---
layout: default
title: "Data Science Discussions"
description: "Comprehensive Exam"
tags : [general]
---

{% include JB/setup %}

# Data Science Discussions

## P-values:  

*Currently there is a monumental debate taking place in the academic fields of statistics, data science, and any field that conducts analysis -- that is looking at the future of statistical inference. The center of that debate is the concept of statistical significance and specifically the role of p-values. Summarize the salient points of that debate and how data sciences and its computational approaches have driven the debate to the forefront. Further, speak to how the outcome of this intellectual debate could influence the future of data analysis practice.*


P-values have indeed been a common feature of statistics, particularly in introductory classes, as it serves as a useful tool for steering students away from varying degrees of the randomness of variation (when analyzing data). However, it can also be argued that due to the complexity of the concept (to teach, to learn, and consequently interpret), it may be time for a change in the computational approaches in data sciences to stop the continual discussion of its use and misuse. Others simply argue that the main issue with the P-value is really dependent on the user; that the main issue is simply based on user knowledge and aptitude. One writer believes that the "practical alternative to the p-value is the correctly used p-value" [@lakens2019practical]. 

However, while alternatives statistics were proposed to the P-value statistics, such as confidence intervals, effect sizes or Bayes factors, there was evidence to support that those methods were marginally indifferent and offered similar interpretations of uncertainty. The main issue for some was that the core of the difficulties being experienced was as a result of research design flaws; that these issues were perpetuated through educating non-statisticians on how to interpret and present the volumes of information that result from analyzing their research data [@Goodman]. It appears that Raymond Hubbard has taken it upon himself to truly question and explore the matters Statistical Significance Testing and P-values applied in various scenarios. These works include works such as "The historical growth of statistical significance testing in psychology—And its future prospects" [@hubbard2000historical], and "The Rise of Statistical Significance Testing in Public Administration Research and Why This Is a Mistake" [@hubbard2013rise]. 

The undercurrent of disagreement resulted in a confusing set of events that impacted the industry; the events that took place, and the decisions that were made, transpired in 2015 to 2016, between the editors of Basic and Applied Social Psychology and the American Statistical Association were abrupt and jarring to the industry. This did, however, question established ideas in the industry and for that, the industry is better for it [@Goodman].

The matter can be said to have begun with, R.A Fisher and the introduction of the P-value concept in Statistical Methods for Research Worker in 1925 [@Goodman]. This was touted to provide users with many benefits. It is said that p-values indicate how surprising the data is assuming that there is no effect. P-values serve as helpful guides against being fooled by randomness separating the signal from the noise in data; instead, they allow to do you to hone in on set details to make inferences with varying degrees of certainty. 

However, as of 2015, the editors of Basic and Applied Social Psychology (BASP) were said to have "effectively banned the use of null hypothesis significance testing (NHST) in manuscripts submitted for publication" [@Goodman]. At the time, there were concerns that if such a ban were to be permanent then it would not only have costly ramifications for academic works that were already in progress (or just concluded) but also create issues for older works. This decision was met with bit positivity and negativity, as some academics praised these efforts, while other staunchly opposed this change by attempting to maintain the use of null hypothesis significance testing. Many editorials were written for both sides of the argument [@Goodman]. 

The American Statistical Association then released a policy statement on p-values outlining their context, the process by which they should be used and their purpose in scientific literature and research as of March 2016. A key statement highlighted that “the statistical community has been deeply concerned about issues of reproducibility and replicability of scientific conclusions" and that “misunderstanding or misuse of statistical inference is only one cause of the reproducibility crisis" [@Goodman]. 

Overall evolution in statistics continues to occur outside of industry squabbles. For example, P-values are still evolving [@blume2018second; @blume2019introduction; @lakens2018equivalence]. Jeffery Blume has recently been making great strides in the statistical world. He believes that there may be a need for a second iteration of the P-value [@Blume]. In order to uphold the tenet of statistics seeks to provide proper verification that a statistically significant result has provided a scientifically meaningful, and ensure that Type I error rates are controlled, the second-generation p-value stands to account for scientific relevance and leverages that this "natural Type I Error controls" [@Blume]. This idea is predicated on the preset values; there is to be a  "pre-specified interval null hypothesis that represents the collection of effect sizes that are scientifically uninteresting or are practically null" [@Blume].

The second-generation p-value is also said to be "the proportion of data-supported hypotheses that are also null hypotheses"; therefore this approach is said to be compatible with null hypotheses, or with alternative hypotheses, or when the data are inconclusive" [@Blume]. Additionally, it is said to bear "proper scientific adjustment for multiple comparisons and reduce false discovery rates"; this change facilitates better handling of the ever-increasing amounts of large datasets, where traditional p-value adjustments viewed as being are "needlessly punitive" [@Blume]. This newer version is suggested as being more transparent, yet rigorous and capable of providing reproducible scientific results (by a priori specifying which candidate hypotheses are practically meaningful and by providing a more reliable statistical summary of when the data are compatible with alternative or null hypotheses) [@Blume].

Blume's work suggests that the P-value does indeed have the potential to change and that it does not need to be completely disregarded. There are many others just like Blume that are currently reviewing existing standards and tools and attempting to find ways to improve the industry. As statisticians, we should always be willing to expect and accept change when there has been sufficient presentation of facts and a streamlined plan of implementation. 

However,  such actions were not what would have been expected from data professionals that usually attempt to be careful with the information that compiles and present to society. The initiating action seemed to be an afront to common decency and respect to boards of authority in the industry, as well as to the members of the larger data community. While this matter did not gain much traction on in new beyond the industry, the discussion points presented from those events should continue in order the field to progress, and so too society. 

Ultimately, I believe the main outcome of this such a debate will indeed lead to industry change, however, this change may be very gradual. I do however see merit in at the very least find ways of improving the way that statistics is being taught, as it is argued that finding alternative will still not be successful in the necessary teaching resources and mentality is not in place.


## Bootstrap: 

*In 2018, Bradley Efron won the International Prize for his work developing the concept of the Bootstrap. The bootstrap procedure is considered a threshold concept that aided in ushering in the computer age of statistical inference. What is the bootstrap, why is it so valuable in assessing uncertainty and why is it considered such a landmark development?*

Bootstrapping is indeed a procedure/tool that can be employed to simplify the performance of statistical inferences. It believed to be quite useful in statistics as it allows for creating confidence intervals and standard errors in situations that would have been deemed problematic in its absence. In other words, it is believed to have simplified the mathematical aspects of performing inferences [@euclidbci; @SmartVision; @teachers; @secondthoughts; @gentlemethod; @recovering; @sciencedirectoverview; @sbiteacher].

The bootstrap method is said to be a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. A bootstrap sample can be viewed as a "smaller sample that is "bootstrapped" from a larger sample" and is predicated on the concept of resampling [@define]. With bootstrapping one can take a small sample, and generalize to that data to a larger population. If one were to use an item in their sample as a surrogate for an item in the general population, one would be able to randomly sample them with replacement to not only generate different data sets but also calculate the difference between the experimental and control conditions. 

Thousands of iterations can be run on this data as long as the base conditions are met. The rationale behind this assumption is that if one randomly samples subjects from the general population, each one of those individuals can be used as a surrogate for any other random selection from the population; that item can indeed be re-drawn multiple times with replacement without issue. Resampled means be then be used to make additional inferences. Many persons like to draw on the example of calculating the confidence interval for the median when discussing Bootstrapping. Without its application, one would need to go through the rather complicated and often time-consuming mathematical calculation to arrive at a sufficient answer. With Bootstrapping, the process is simplified.

The Central Limit Theorem has been established as the distribution of the mean of a random sample from a population with finite variance is approximately normally distributed when the sample size is large, regardless of the shape of the population's distribution. It is important to note that the bootstrap distribution of a statistic, based on the resamples, represents the sampling distribution of the statistic; therefore, Bootstrapping can be used to easily understand Central Limit Theorem as it can perform multiple iterations of select items from the data [@minitab].

Statistics is said to be the branch of mathematical engineering which studies ways of making inferences from limited and imperfect data; there is indeed an air of uncertainty to this field, and Bootstrapping is the perfect tool to address. Bootstrap is said to be a general tool for confidence intervals and an assessment of uncertainty. However, it should be noted that bootstrapping may not be infallible; some have stated that it may encounter possible issues with uncertainty where the "bootstrapping confidence interval wrongly indicates that there is a decent chance that p=0" [@recovering].

To better clarify, I sought out explanations from the creator. Bradley Efron once attempted to summarize Bootstrapping, by acknowledging that one can deal with statistics on two levels; one level allows for the creation of algorithms to estimate an entity, and the second level attempts to figure out how accurate those algorithms. Stated another way, the first level is attended to by the algorithm which is answering the question the researcher is interested in; Bootstrap is usually attended to the second-level analysis (that being the determination of how accurate the answer). 

It is indeed important to note that the Bootstrap is always in a supporting role in the process, rather than the leader. It is meant to be a tool of assistance to bridge the gap between levels one and two; of estimation and accuracy. He explained that while the traditional way of handling these details would be mathematics, the problems being posed now are increasing in their complexity, and the datasets have drastically increased in size. Efron stated that Bootstrapping is viewed as a way of utilizing the computer's power to answer the questions of accuracy; without technology, these processes would have been highly time-consuming.

He spoke of the main benefit being Bootstrap allowance for mathematical tractability. This may be part of the reason why it is considered a threshold concept, as it has indeed served to revolutionize the way that data was being approached. More than most careers, Statistics are always working with iterations of data to find ideal models or solutions, therefore, the act of repeating repetitive processes would indeed be time-consuming, and thus slow down the process of scientific experimentations and breakthroughs. 

Not only did does this process improve the productivity and accuracy of routine experimental processes, but t also frees up time for new experimentation. Therefore Bootstrapping allows users the benefit of the freedom to pursue the creation of a highly useful but complicated algorithm, and utilize massive datasets, all while being comforted by the fact that their computer can produce an effective measure of accuracy. This time-savings in calculation is indeed invaluable to the data science community where calculations and number crunching is key. Statisticians would then be free to challenge themselves and approach new problems confidently based on the reassurances provided by Bootstrapping.

Interestingly, the teaching of such a concept is increasingly being encouraged at earlier stages in the education of statistics. Bootstrapping's mathematical tractability is most likely what cemented its significance as a threshold concept when applied to an educational setting. Bootstrapping would serve to tackle topics and concepts that are often overlooked in the teaching of statistics, due to their inherent complexity and time-consumption. 

Not only does Bootstrapping serve to tackle complex calculation issues, but it also serves as a teaching aid to allow for the exploration of complex calculations, but also for the consistent teaching of concepts (rather than dropping them off simple explanation does not fit into the time constraints of a classroom). For instance, topics dealing with medians at advanced levels are almost always glazed over. Some have attempted to create works that encourage teachers to present this concept to introductory-level statistical courses at colleges; Tim Hesterberg's work "What Teachers Should Know about the Bootstrap: Resampling in the Undergraduate Statistics Curriculum" is a perfect example of this [@teachers]. Bootstrapping also has great applications in the modern era where companies with access to "big data" are attempting to find value in these possessions. The utility can range from medical areas when dealing with cases of rare diseases (thus small sample sizes).

It should be noted that with the teaching of bootstrapping, Tim Hesterberg suggested a few cautions [@forget]. It is said that the percentile bootstrap is to be viewed as a stepping stone (as it is often too unreliable); instead, it can be used as a base for others. Notably, there are better bootstrap methods than like *bootstrap t-interval*. Additionally, bootstrap opens the door to more statistics (like the standard deviation and IQR). Moreover, teachers would need to establish appropriate conditions for the bootstrap (this is to ensure proper appreciation of this threshold concept). Finally, teachers were advised that "shifts in pedagogy are costly, so proper preparation of motivating materials for a change needs to be created, as well as, proper teaching materials.

Many believe that Bootstrapping gets its name from the fact that it is a very helpful tool that aids the user in pulling themselves out of a mathematically difficult situation; the *Bootstraps* serve as a means of getting the user out of state lacking statistical inferences. Interestingly, in the U.S the concept of pulling yourself up by your bootstraps implies that one is attempt succeeding or elevating yourself without any external assistance; the act of pull yourself up by your bootstraps is highly physically impossible [@phrase]. The phrase has been around a while going through several iterations in meaning until landing on the current understanding - the impossible can become possible if you try hard enough. 

Whether not this was Bradley Efron's main point with this procedure, the phrase's application the realm of statistics does appear to be apt as one can approach a once challenging, with relative ease. It is said to have been created back in 1979, but Bradley Efron's first official publications using the term Bootstrapping was his 1988 “ Discussion: theoretical comparison of bootstrap confidence intervals" within *The Annals of Statistics*. It was very short, barely four pages long but was indeed the start of a dynamic shift in how many viewed statistical possibilities.


## Noam Chomsky:

*Many current machine learning techniques (e.g., deep learning) being used to analyze natural language seem to ignore Chomsky's idea of "deep structure". Do you think it will be possible to make major advances in the analysis of natural language in the next few years without considering the role of "deep structure"?*

Noam Chomsky is known as an American philosopher, cognitive scientist, historian, social critic, and political activist, and most significantly a linguist; his works span decades and they are still impacting the decisions that professionals make in the many areas of study that he pursued. He is often affectionately referred to as "the father of modern linguistics" and has been a great contributor to the field of Chomsky field of cognitive science (and played a role in the development of analytic philosophy). 

As technology seemingly drastically changes every day, the significance of older contributions is often questioned for relevance. Many of Noam Chomsky's ideas focus on the natural aspect flow and order of things; it is based on instinct and nature. These are what his ideas thrive on. However, in the field of machine learning, it may not be as easy to apply these natural, intuitive deals over to such technology. This, therefore, can evoke the age-old image of the "chicken and the egg"; meaning would it not be wiser to wait until the technology has developed or evolved enough to fully appreciate the application of his techniques or is only possible to develop the technology that allows for it to do so.


Noam Chomsky's deep structure refers to the deep/submerged aspect of a linguistic expression; it is a theoretical construct that seeks to connect several related structures - the underlying relationship intended to beyond the cursory details. The meaning that we attribute to them. When discussing "deep structure" Noam Chomsky is noted as referring to the inherent "underlying structure of grammatical relations and categories, and certain aspects of human thought and mentality are essentially invariant across languages". He explored and questioned human thought and mentality at its core. He not only sought to have to study this in English but across various languages and cultures to see if they were the same at their core.

Machine learning techniques (e.g., deep learning) are increasingly being utilized to explore the most difficult aspects of human existence. It can identify patterns and note details that the human mind or body is incapable of producing to the same degree of speed or accuracy. Linguistics appears to be an area of increasing interest, however, it is indeed one of the more difficult subjects to work with because a lot of it is inherently based on unique human experiences and cognition. Therefore, when analyzing natural language, many seem to ignore Chomsky's idea of "deep structure"; I would argue that they may have indeed attempted to include his considerations, however, many may find it too difficult to proceed forward.

Ultimately, I do believe that it will be possible to make major advances in the analysis of natural language in the next few years without considering the role of "deep structure", however, it may not be recommended. Rather I would always recommend its consideration in the hopes of sparking development. Noam Chomsky's thoughts on the deep structure may just be the unique key in unlocking major turning point in the field of machine learning and natural language analysis.


## Data Science Domains: 

*Data Science has its origin in multiple domains: artificial intelligence (including machine learning), applied mathematics, experimental design and analysis, business intelligence, mathematical modeling, forecasting, and others. As the field of data science develops in the next five years, which domains do you think will increase in importance and use, and which domains will become less important. What do you think will bring about these changes?*

While Data Science does bear multiple domains (artificial intelligence (including machine learning), applied mathematics, experimental design and analysis, business intelligence, mathematical modeling, forecasting, and others), they do not survive in as collaborative of a fashion as industry outsiders may think. Data science is a wide umbrella that pushes for the advancement of technology, yet not all of the parts move in unison or directly interact with each other intuitively. By its very nature, some domains progress faster than others, and so feed the others, nourish them with new concepts or tools, that will, in turn, boost their advancements. 

For example, breakthroughs in applied mathematics might have an impact on artificial intelligence (including machine learning); mathematical modeling, might have an impact on forecasting. Additionally, industry groupings are not as clear cut, since one's skills can be universally applied to various industries; for example, careers in mathematical modeling, and forecasting may be found on Wall Street or in an investment bank.

Most industry chatter would encourage one to believe that the domain with the highest demand would be artificial intelligence, however, the longevity of this attention is not guaranteed. Since data science is still evolving, it may be difficult to encounter straight forward metric that investigate every facet of the industry. Therefore there are various tools and cues one could use to make assumptions about the future of the different domains. 

For example, it may be useful to review governmental salary information (income) to obtain some idea of the market need for the domain. Another method might include reviewing industry reports generated by industry experts such as Stackoverflow, or others. Regardless of the assumed dominance of certain domains, as people of science, and appreciators of technology, we must keep in mind that projections, even as soon as five years, can prove to be inaccurate due to the element of breakthroughs.

As of 2019, it was said that there are more vacant positions than the number of Data Scientists in this world [@Flair]. Data Science jobs were said to have been the highest paid in the IT industry [@towardsjobs]. In the same year, Data Science was hailed as the “sexiest job of 21st Century” by Harvard Business Review [@Flair]. 

It is believed that the new driving force behind industries is Data, and continued success hinges on the lack of 'data-literacy' in the market; therefore as long as there is data, there is potential for a domain to grow. However, it should be noted that some domains focus specifically on calculations of data, or tool building, and therefore may not be able to have tangible metrics to be measured with the expectation of growth. 

It does appear that careers in the industry that require high-levels of education, creativity or decision making do appear to retain the highest salaries. According to bls.gov, as of 2018, the median pay for the highest paying job in "Computer and Information Technology Occupations" would be for Computer and Information Research Scientists with $118,370 (with a Master's degree) [@blsoccupations]. Unfortunately, such a grouping is not sufficiently clear as this could include multiple domains, like artificial intelligence (including machine learning), experimental design and analysis, and forecasting. 

However, it was stated that "employment of computer and information research scientists is projected to grow 16 percent from 2018 to 2028, much faster than the average for all occupations. Computer scientists are likely to enjoy excellent job prospects because many companies report difficulties in finding these highly skilled workers" [@blsoccupations]. Interestingly, according to bls.gov, of the "Fastest-growing occupations" (20 occupations with the highest percent change of employment between 2018-28), it is currently that of "Solar photovoltaic installers" with a growth rate of 63%  [@blsfast].

Market research analysts and marketing specialists ranked 14th out of 20 on the "Most new jobs: 20 occupations with the highest projected numeric change in employment". They were projected to grow "20 percent from 2018 to 2028, much faster than the average for all occupations. Employment growth will be driven by increased use of data and market research across many industries" [@blsmarket]. Of the "Highest Paying Occupations" no occupations related to the data science domain made it into the top 20 ranks (Psychiatrists topped the list).

AI appears to have the strongest case for an increase in over the next five years due to image recognition technology. Many governments will attempt to utilize these as part of internal security measures, as well as global monitoring (defense). Attempting to utilize these the technology for medical advancements (image recognition of medical data) will lead to earlier and more accurate disease detection. 

Additionally, the world appears to be enthralled with the idea of deep-fakes and counteracting them; businesses may also attempt to utilize this technology in the hopes of creating new interesting products. Self-driving cars and autonomous robots are still topics that generate quite a deal of interest, and so has room to develop. 

If I were to speculate I would assume that applied mathematics might end up slowing down or maintaining a slow pace in the next few years. While there is interest building for STEM careers, pure and applied mathematics does not appear to be as appealing as the other domains. Career prospects are also quite limited and most degree holders end up underemployed, the are unable to find many directly related jobs. I do not see the need going away, but I would expect continued slow growth.

While many employees have become accustomed to discussions of that rapid pace of AI development, and the negative impact it might have on the job market, one study found that 49% of respondents felt that artificial intelligence (AI) and automation, would have no impact on their job. While there may even be concerns that industry insiders may lose their high-skill level jobs to technology, many sources suggest that this is not the case; that jobs requiring a high degree of creativity and skill will endure beyond five years into the future. Since technology does serve to reduce the number of menial and repetitive tasks that one may have to endure, it would only benefit employees. However, not all areas of Data science may be impacted by the improvements in AI *equally*, nor the employees associated with these industries
 
Ultimately, I foresee growth in all industry domains. As we continue to use more and more technology, it will require more employees to create and maintain them. Data science will supply the market with its various eclectic domains.



## Computer Programming Language Trends:
*It is hard to displace an existing human language when one is already in place. In as much as, two thousand years ago, the Romans wanted the countries they conquered to communicate in Latin, many in those countries were already using Greek to communicate between cultures simply because the same territories had been conquered three hundred years earlier by Greece. In the same light, replacing a common computer programming language is also difficult. It doesn't take hundreds of years, but it still takes time. One exception to this process seems to be occurring in Data Science. In the few years that you have been studying analytics and data science, the dominate language has just shifted from R to Python in the last year. How do you think this happened? What do you think will happen in the future?*


As one that appreciates data, I concede the obvious merit in the proposed claim that Python has indeed surpassed R in being the industry dominant computer language. It may even be useful to address the underlying appeal of Python over R to comprehend the drastic growth. However, I do disagree with comparisons drawn between the domination shift of these languages to that of traditional natural languages. While I do believe that the near future does seem positive for Python's upward projection, paired with R's slide in dominance, like any language in the long term, it may wane in popularity when a newer more capable technology appears. It is said that *"integration into a single "linguistic community", which is a product of the political domination that is endlessly reproduced by institutions capable of imposing universal recognition of the dominant language is the condition for the establishment of relations of linguistic domination"* [@speakinggreek]. Therefore, a language's vitality, be it a traditional natural language or computer language, is intrinsically linked to the strength of its supporting community.

A spoken language can be defined as the "accrual of shared meaning constructed within neurobiological systems that develop under the influence of a genetic program and social experience" [@ramacciotti2019language]. A programming language can be noted to possesses syntax and semantics (where syntax is related to the "spelling of the language’s programs", semantics is related to the "meanings of the programs"); here a language’s syntax is noted to formalized by a grammar or syntax chart in its manual [@schmidt1996programming]. The term "languages" is applied to the technology industry as it allows programmers to closely model their communication of task requests to their tools most intuitively and naturally [@hromkovivc2016combining].

However, the true differences between traditional natural languages and programming languages may lie within the ease with which evolution can take place. With natural languages, these changes tend to evolve as persons communicate, while, ever so often, pushing against the established "rules" of how the language has historically existed [@greenhill2010shape]. Programing languages may not suffer the same issues of "language attrition" or drastic shifts when a new language is learned by an individual due to the rigidity of the coding structure; in scenarios where a new natural language is introduced the knowledge of their native language (L1) may begin to diverge from that of monolingual speakers, however, coder may experience a form of forced "balanced bilingualism", as coding divergence would simply lead to inexecutable code [@schmid2018predictors; @hemandez-chavez_language_1978]. However, with programming languages, while some may independently tinker with established open-source code, there tends to be gatekeepers and regulators associated with the distribution implementation of code. This rigorous structure is necessary to ensure standardization, proper version control, and reproducibility of results [@schmidt1996programming].

The learning of programming languages, like languages, can be impacted by environmental, experiential, attitudinal and individual factors [@schmid2018predictors]. Languages thrive in environments where they are heavily expressed and speakers can find helpful resources to support fluency.  The most utilized factor for evaluating the vitality of a language is its transmission from one generation to the next. A language is regarded to be in danger when its "speakers cease to use it, use it in an increasingly reduced number of communicative domains, and cease to pass it to the next generation" [@drude2003language]. It is endangered when it is on a path toward extinction - without adequate documentation, it can never be revived [@drude2003language]. Both R and Python are established entities, are still highly used, and are well-documented; therefore, neither would be considered to be in danger nor endangered (despite Python's dominance) [@incrediblegrowth; @robinson_why_2017; @python_2019; @Octoversereport; @InteractiveIEEESpectrum; @TIOBEIndex]. While it is a bit difficult to quantify these trends for a spoken language it is inherently non-digital, although now steps are being made to observe its vitality and evolution from the degree of activity on social media platforms, and also investigating sentiments related to them; Twitter data of the Irish dialect and other endangered languages is being closely monitored. In a way, the vitality of programming languages can also be gauged based on their online presence [@indigenous_tweet; @TLynn ].

Stack Overflow (SO) boasts that it has become the largest, most trusted Q&A online community for coders to develop or reinforce their skills, share their knowledge, and build their careers [@stackoverflowabout]; moreover, this company has expressed their interest in utilizing their in-house data to provide crucial insight into the worldwide software development community, and so reports on data are often made. Many other similar entities exist in the space that reports such industry progress, like Github's Octoverse report, the Interactive IEEE Spectrum's report (an authority in applied science and engineering), and the TIOBE Index (a programming Community index that serves as an indicator of the popularity of programming languages) [@python_2019; @Octoversereport; @InteractiveIEEESpectrum; @TIOBEIndex].  Quantifiable industry chatter could be used to confirm and explain Python’s dominance.

As of 2017, an internal SO report highlighted the appeal of Python's versatility of application; similar reports by various external parties all produce the similar indications of Python's incredible growth [@incrediblegrowth; @robinson_why_2017; @python_2019; @InteractiveIEEESpectrum]. In the same year, Python was part of Forbes' top ten "Technical Skills With Explosive Growth In Job Demand" due to a 456% increase in demand from the prior year [@Forbestop].  In 2018, the TIOBE Index noted that Python had the highest rise in ratings in a year; Python also received a third-place award for notable growth in 2019 [@TIOBEIndex]. The 2019 Octoverse report also reflected its significant industry growth [@python_2019; @Octoversereport].  While R also noted growth during these times, the pacing was drastically different.

SO data visualization clusters revealed that most issues (particularly those related to data science and machine learning) were notably capable of being resolved through the application of Python tools [@robinson_why_2017]. Moreover, Python's rate of growth occurred evenly across most industries [@incrediblegrowth; @robinson_why_2017; @python_2019]. Python is increasingly sought in areas such as electronics, manufacturing, software, government, and even finance; this language is particularly important to academic applications, such as university research programs [@robinson_why_2017]. While languages like Rust, R, and Go noted faster growth than Python in 2018, monthly traffic growth in the years since 2012 was lesser than Python [@incrediblegrowth]. Another 2017 SO report noted the Python tag was visited, about twice as often in high-income countries as in the rest of the world; however, R was stated to be visited three times as much under the same conditions.  R and Python were more common in "wealthier industrialized nations" that allow for scientific research to account for a larger portion of the economy and that programmers would likely hold advanced degrees (thus the focus on R) [@robinson_tale_2017; @stacktrends].

Python serves as a general-purpose language, capable of being utilized beyond simple websites; Google’s YouTube was also written in Python [@incrediblegrowth]. It also allows for writing backends for websites [@samepython; @robinson_why_2017; @incrediblegrowth]. It is viewed as being "an easy and elegant language to learn to program" for beginners [@gries2017practical; @hromkovivc2016combining; @grandell2006complicate; @stajano2000python; @robinson_why_2017]. In fact, Python’s future growth is a bolstered by Google’s recent launch of a Python, not R, educational series [@google_python; @gclass].

In fact, it is Python's pace in developing its data science uses that has drastically driven its popularity (although, closely followed by the areas of system administration and DevOps engineering) [@samepython; @robinson_why_2017; @incrediblegrowth].  The academic industry often has the highest volume of Python traffic; data science, machine learning and general academic research became a great portion of the programming community, and thus boosted Python's appeal and dominance  [@gries2017practical; @hromkovivc2016combining; @grandell2006complicate; @stajano2000python; @robinson_why_2017].  While both R and Python have research packages and services, Python’s rate of product improvements in areas of research outpaces R; Python's Pandas package appears to be essential to this surge in research communities [@pandastack; @robinson_why_2017]. In my 2018 co-publication, *"Survival Analysis within Stack Overflow: Python and R"*, Python had roughly four times the total number of posts, yet followed a similar scale adjusted growth curve as R  [@lord_survival_2018].

Python is described as having a high readability rating, thus allowing for novice users to readily intuit interesting executables [@learneroo; @robinson_why_2017]. Many appreciate its uncommon indentation feature that ensures ease of code navigation [@learneroo]. Moreover, Python can be said to be a solid choice of persons that are inquisitive about programming yet lack a specific goal.

R is best known for its application to academia and research, however, it is this very quality of specialization that may be problematic to non-industry experts [@samepython; @robinson_why_2017; @incrediblegrowth]. One does not have to be a researcher to see the value in Python, but R requires that appreciation. Some do however believe that while Python aims to be an ideal scripting language, it sacrificed speed and verifiability for productivity and portability [@samepython].
Python and R, therefore, appear to be at differing stages in their evolutionary capabilities, where Python is being recognized as an official language, and R is more like a creole or pidgin - functional yet limited [@learneroo]. One scholar suggested that the difficult part of learning to program lies with the fundamentals - adapting to a new language is only a matter of syntax; the key is having a good support network in the early learning stages. It may be helpful to select a programming language based on what your immediate community utilizes; increasingly, that language is Python.

Language dominance is stated as being the relative proficiency of the two languages within the same individual [@shen_interactive_2014; @bedore_measure_2012]; with the programming languages R and Python, dominance may be influenced by the market demands and resource availability but does not guarantee a complete language shift. R’s survival relies on its ability to prove its utility; it should ensure high quality and quantity support to users, and growth should occur with the addition of supplemental packages. Languages are indeed dependent on the numbers of users/speakers, their fluency, and their expression needs (how relevant or essential this language is for the user's daily life). To draw upon the initial imagery, it should be noted that the "ideal was when the Roman was fluent in both Latin and Greek" [@speakinggreek]; therefore, the coder should be encouraged to attempt both R and Python as there are many inherent benefits to being multilingual.



## Futurists: 

*There is a subset of futurists who believe that A.I. agents will soon be able to handle the bulk of the work done by data analysts and that this will, in effect, lead to the unemployment of the bulk of data scientists and analysts Do you think they are correct?*

As of January 14, 2020, Sheelah Kolhatkar of the New Yorker published an article entitled *Could New Research on A.I. and White-Collar Jobs Finally Bring About a Strong Policy Response?"*; in this work, she highlighted the fact that new findings suggested the high-skill level "office jobs" were are now less secure from various forms of automation lead redundancies. While in the past the public may have been able to passively consider the implications of technology's impact on the workforce, positions of doctors and even computer programmers are being warned of possible significant shifts in their employment [@Kolhatkar; @brookings]. However, in 2017 IBM predicted that demand for Data Scientists would soar 28% by 2020, and according to current U.S. Bureau of labor statistics data employment of computer and information research scientists is "projected to grow 16 percent from 2018 to 2028" (much faster than the average for all occupations) [@ForbesIBM; @blsjobs]. Computer scientists, mathematicians, and statisticians are likely to enjoy excellent job prospects because many companies report difficulties finding these highly skilled and educated workers, and Businesses will need these workers to analyze the increasing volume of digital and electronic data. [@blsjobs; @mathjobs; @towardsjobs]. Regardless of the validity or accuracy of their claims, futurists simply serve as early warning systems to allow society to truly reflect on a given scenario, and therefore establish course correction mechanisms. Data science and analytics are precisely the professionals capable of assessing, reporting and recommending ways to address such a serious matter; they are professional problem-solvers. Simultaneously global unemployed seems highly improbable as countries vary in their rate of adoption of technological advancements. Moreover, it would be uncharacteristic professional malpractice by an entire industry to simply abide by such an occurrence. Moreover, the prospect of a *singularity* should not be met the public languishing about their future, but rallying to meet the challenge motivation and resources a different outcome can be achieved.

The topic of "futurists" can be a bit tricky to tackle as the earliest proponents of this concept were seen as radical, problematic and even crazed, and the movement itself if a bit eclectic; this can make consensus on theories and principles a bit difficult to identify.  Thomas Frey suggested twelve "Common Ground perspectives" that fellow futurists share [@FeyFutureHall]. One is to seek to improve contemplation of the future, to be better prepared for what lies ahead; individuals should understand that they are living in unusual times and the concept of "normal" is shifting [@FeyFutureHall]. Fey states that one's understanding of the future should direct impact today's decision making [@FeyFutureHall]. He believes that those who study the future are conditioned to think in terms of multiple alternatives, possibilities, and probabilities [@FeyFutureHall]. Futurists are to be motivated by change, and actively participate in creating the future; they are generally practical and pragmatic as they construct long-term perspectives (policy-oriented works where possible) [@FeyFutureHall].  They are not content idly to describe, predict, or forecast, but want to actively participate in the world as it transforms [@FeyFutureHall]. 

Fey suggested that even businesses can greatly benefit from accepting the futurist perspectives in their long-term planning; that *future studies* is essentially *systems thinking* [@FeyFutureHall]. Finally, Fey cautioned that persons are all seeking an improved outlook for the future, yet the essential components for doing so are not always glaring, however, it is their responsibility to attempt its discovery. He stresses that at the center of an improved future is one that is "sustainable, preserving the essence of humanity and the environment we live in, inclusive of culture and technology" [@FeyFutureHall]. 

A few notable futurists that were exemplars of the aforementioned perspectives date as far back as 87 BC; Archimedes was an ancient Greek mathematician, physicist, engineer, inventor, and astronomer [@FeyFutureHall]. In the 1400's Leonardo da Vinci was an Italian visionary well regarded for his futuristic designs, inventions, paintings, sculptures, architecture, science, music, mathematics, and engineering among other things [@FeyFutureHall]. Steve Jobs was a modern futurist from American; he was an entrepreneur, co-founder, chairman, and chief executive officer of the highly successful company - Apple Inc. [@FeyFutureHall]. Isaac Newton, Ada Lovelace, Marie Curie, Albert Einstein, Nikola Tesla, Carl Sagan, and John von Neumann are also well-recognized futurist that have made substantial contributes towards human progression (albeit at times with unintended negative impacts) [@FeyFutureHall].

It should be considered, however, that at times the works of these futurists may enhance social unease concerning technological advancements; unintentionally or not, these works have caused such stark changes to society's perceptions of their reality and possible futures, that it manifests as "automation anxiety" and possibly even existentialism [@fromknowledgeDonovan; @ExistentialGoldbach]. For example, the futurist, Alan Turing was regarded as a "Pioneering British computer scientist, mathematician, logician, cryptanalyst, and theoretical biologist, and notably invented the *Turing Test*. His 1950 paper, *Computing Machinery, and Intelligence* proposed a game where a human judge was given a text conversation with unseen players and had to evaluate their responses; and to succeed a computer would need to be capable of replacing one of the players, without substantially altering the results [@OGtTuring; @videoturning]. This game exploring considering a computer intelligent if the conversation could not be easily distinguished from a human's [@OGtTuring; @videoturning]. While the attempt was not without issue, the very act of pursuing this, and presenting such a concept was a bit disturbing and questionable to some (including the aforementioned Ada Lovelace) [@yearslaterSaygin; @hayes1995turing; @OGtTuring; @videoturning; @turingHarmless; @standfordturing]. While Turning also unsuccessfully predicted that by the year 2000, machines with 100 megabytes of memory would be able to easily pass his test, his concept is increasingly nearing realization [@yearslaterSaygin; @hayes1995turing; @OGtTuring; @videoturning; @turingHarmless; @standfordturing].

The term "singularity" was shortly introduced; it theorized that based on the pace of technological advancements, society would eventually arrive at a point where the status of human affairs as we are accustomed would cease to exist [@videosingularity; @Kurzweilsingnear; @ZimmermannActualization; @AIsingularity]. This idea as been attributed to the futurist John von Neumann before his death in 1957, but several persons have taken up his mantle [@videosingularity; @Kurzweilsingnear; @ZimmermannActualization; @AIsingularity]. For example, Ray Kurzweil and Cory Doctorow to continue these discussions in terms of a technological event horizon [@videosingularity; @Kurzweilsingnear; @ZimmermannActualization; @AIsingularity].

As persons continue to consider the gravity of such a technological milestone, the works of futurists in the field of literature are continuously reviewed; this review can breed feelings of negativity and disenchantment towards technological advancement. Jules Verne, H. G. Wells, and George Orwell created works that had a great impact on the literary genre of science fiction [@FeyFutureHall]. Their works have inspired more modern interactions of science fiction that tackle the topics of singularity and the varying degrees of associated technophobia and technophilia; the *Terminator* franchise of movies, and the *Blackmirror* and *Westworld* TV shows present scenarios where humans, in their zealous pursuit of advanced artificial intelligence solutions to tasks, have relinquished a great degree of control and privacy; the outcomes usually overwhelmingly grim [@janetAtlantic; @nearIndependent; @ExistentialGoldbach; @godwalkBurz; @Terminator]. 

Ultimately, despite the positive industry outlook from the government, data scientists and analysts are not immune to concerns of employment stability when faced with the rapid pace of artificial intelligence implementation and evolution. Such employees are acutely aware of these advancements due to their direct roles in its creation. However, economists like  Heidi Shierholz, who has focused on the labor market, and scholars like Erik Brynjolfsson, and Tom Mitchell, have been able to reassuringly suggest, that despite the rapid speed of advancement, there is no industry consensus on tasks where machine learning systems profitably excel [@robotsShierholz; @brookings]. Therefore, there is little agreement on the expected impacts on the workforce and the economy at large. They posit that AI is a very different technology than earlier types of automation, and is mostly going to impact different parts of the workforce in different ways; that because so little is known about AI, compared to other types of automation, it seems ambiguous, and its impact is therefore currently confined [@robotsShierholz; @brookings].

At this point, the idea of bulk unemployment seems improbable. For now, AI is projected to continue providing industry jobs, and any unemployment shifts would be gradual [@promisingjob]. A StackOverflow report highlighted that careers requiring advanced degrees in scientific research and programming thrive in wealthier industrialized nations [@robinson_why_2017; @stacktrends]; such fields would consist of a larger share of the economy, therefore such a labor collapse in that industry would have global economic ramifications. The role of governments would be crucial in ensuring proper safety nets for re-training or discussion of options for financial assistance [@MIT; @UBI]. Moreover, data science skills are in high demand; regardless of the creation of an advanced tool, its application may not immediate globally adopted. If wealthier parts of the world are to afford to reduce this workforce, gradually opportunities would arise in countries unable to afford the new technology yet still requiring high-skill workers. However, that is not to say simply moving to an area of dire need is the only way to maintain employment in the data science field. As practitioners and appreciators science, we must also appreciate that nothing in the field is intended to be permanent. Everything is to be questioned to promote advancement- even if one's position is the thing that becomes redundant. 

Redundancy can be seen as an opportunity for creativity, reimagining, reforming and recycling entities into something more profound and useful to society. Jaron Lanier and Paul Root Wolpe push back on any fatalistic perceptions related to the "singularity"; they somewhat demonstrate authentic pragmatic futuristic perspectives by encouraging thoughtful dialogue and planning [@bigthink; @faithproquest]. Data scientists and analysts with lingering concerns should be encouraged to embody the aforementioned pragmatic *futurist perceptions* to address this issue as they are arguably perfectly positioned to shape their futures, through collaborative planning and policymaking.  Perhaps we will relinquish the *tile* of data scientists and analysts, and realize that these roles can be renamed or recreated yet still utilizing the same skillset. The same way hackers have created occupations as security advisors, or it is hoped that these roles can perhaps gently transition into something more senior in the field, if only under a different title [@whitehathacker].
